{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trials_total(version: str, cues: list):\n",
    "    \"\"\"Create trials for the Episodic extinction experiment, using 5 different stimuli for each trial:\n",
    "        A context, two narrative stimuli (NSs), a conditioned stimulus (CS) and one unconditioned stimulus (US), with an associated sound.\n",
    "        \n",
    "        The experiment has 10 different conditions with 40 trials in total, thus each condition has 4 trials.\n",
    "        In half of the trials, the US is negative, in the other half it is neutral.\n",
    "        \n",
    "        On day 2, two conditions are omitted as a control and the remaining 8 conditions are repeated. \n",
    "        In half of the repeated trials, the original context is presented, in the other half a new context is presented.\n",
    "        \n",
    "        On day 3, all the stimuli are CSs are presented again as a cue. Memory tests are performed using this cue.\n",
    "        \n",
    "        As an overview for the conditions:\n",
    "        condition 1: continued conditioning, negative US\n",
    "        condition 2: EXT, negative US\n",
    "        condition 3: control, negative US \n",
    "        condition 4: EXT, neutral US\n",
    "        condition 5: continued conditioning, neutral US\n",
    "        condition 6: control, neutral US\n",
    "\n",
    "        \n",
    "        ToDo: Add \"\" to every value in the dictionary\"\"\"\n",
    "\n",
    "    # Define the index possibilities of the stimuli\n",
    "    CS_i = list(range(36))\n",
    "    US_A_i = list(range(18))\n",
    "    US_N_i = list(range(18)) \n",
    "\n",
    "    # Create lists with all stimulus \n",
    "    CS = sorted(glob.glob('stimulus_files/CS/*'))[:36]\n",
    "    US_A = sorted(glob.glob('stimulus_files/US/negative*'))[:18]\n",
    "    US_N = sorted(glob.glob('stimulus_files/US/neutral*'))[:18]\n",
    "    US_sounds_A = sorted(glob.glob('stimulus_files/USsounds/negative*'))[:18]\n",
    "    US_sounds_N = sorted(glob.glob('stimulus_files/USsounds/neutral*'))[:18]\n",
    "\n",
    "    # Remove the directory from the values within the lists\n",
    "    CS = [stim.split(\"/\")[-1] for stim in CS]\n",
    "    US_A = [stim.split(\"/\")[-1] for stim in US_A]\n",
    "    US_N = [stim.split(\"/\")[-1] for stim in US_N]\n",
    "    US_sounds_A = [stim.split(\"/\")[-1] for stim in US_sounds_A]\n",
    "    US_sounds_N = [stim.split(\"/\")[-1] for stim in US_sounds_N]\n",
    "\n",
    "    # Define the empty data dictionaries for each day\n",
    "    stim_day1 = {\n",
    "        'start': [], \n",
    "                        'CS': [], \n",
    "        'delim1': [], \n",
    "                        'US': [], \n",
    "        'delim2': [], \n",
    "                        'US_sound': [], \n",
    "        'delim3': [], \n",
    "                        'condition': [], \n",
    "        'delim4': [], \n",
    "                        'epsiode_nr': [], \n",
    "        'delim5': [], \n",
    "                        'valence': [], \n",
    "        'end': []\n",
    "    }\n",
    "\n",
    "    stim_day2 = copy.deepcopy(stim_day1)  # Create a copy of the day 1 structure for day 2\n",
    "\n",
    "    stim_day3 = {\n",
    "        'start': [], \n",
    "                        'CS': [], \n",
    "        'delim1': [], \n",
    "                        'US': [], \n",
    "        'delim2': [], \n",
    "                        'US_sound': [],\n",
    "        'delim3': [],\n",
    "                        'US_cue': [], \n",
    "        'delim4': [], \n",
    "                        'condition': [], \n",
    "        'delim5': [], \n",
    "                        'epsiode_nr': [], \n",
    "        'delim6': [], \n",
    "                        'valence': [],\n",
    "        'end': []\n",
    "    }\n",
    "\n",
    "    # Additional dictionaries without 'start', 'delim', or 'end'\n",
    "    stim_day1_clean = {key: [] for key in stim_day1 if key not in ['start', 'delim1', 'delim2', 'delim3', 'delim4', 'delim5', 'end']}\n",
    "    stim_day2_clean = {key: [] for key in stim_day2 if key not in ['start', 'delim1', 'delim2', 'delim3', 'delim4', 'delim5', 'end']}\n",
    "    stim_day3_clean = {key: [] for key in stim_day3 if key not in ['start', 'delim1', 'delim2', 'delim3', 'delim4', 'delim5', 'delim6', 'end']}\n",
    "\n",
    "    episode_nr = 1  # Fixed episode number for all trials\n",
    "    # Fill the trials for each day using two for loops, to make 10 conditions with 4 trials each\n",
    "    for i in range(6):  # Repeat 10 times for 40 rows (10 * 4 = 40)\n",
    "\n",
    "        for j in range(6):  # Each iteration adds 10 rows\n",
    "\n",
    "            # Randomly select indexes for each stimulus set\n",
    "            index_CS = CS_i.pop(random.randint(0, len(CS_i) - 1))\n",
    "            \n",
    "            for List, CleanList in zip([stim_day1, stim_day2, stim_day3], [stim_day1_clean, stim_day2_clean, stim_day3_clean]):\n",
    "                List['start'].append('{\"')\n",
    "                List['delim1'].append('\", \"')\n",
    "                List['delim2'].append('\", \"')\n",
    "                List['delim3'].append('\", \"')\n",
    "                List['delim4'].append('\", \"')\n",
    "                List['delim5'].append('\", \"')\n",
    "                List['end'].append('\"},')\n",
    "                if 'delim6' in List:\n",
    "                    List['delim6'].append('\", \"')\n",
    "\n",
    "                List['condition'].append(i + 1)\n",
    "                List['epsiode_nr'].append(episode_nr)\n",
    "                CleanList['condition'].append(i + 1)\n",
    "                CleanList['epsiode_nr'].append(episode_nr)\n",
    "                \n",
    "                List['CS'].append(CS[index_CS])\n",
    "                CleanList['CS'].append(CS[index_CS])\n",
    "\n",
    "\n",
    "            # Alternating between US_A and US_N for each 5 iterations, and add US sounds for day1 and day2\n",
    "            if i < 3:\n",
    "                index_USA = US_A_i.pop(random.randint(0, len(US_A_i) - 1))\n",
    "                for List, CleanList in zip([stim_day1, stim_day2, stim_day3], [stim_day1_clean, stim_day2_clean, stim_day3_clean]):\n",
    "                    List['US'].append(US_A[index_USA])\n",
    "                    List['valence'].append(1)\n",
    "                    List['US_sound'].append(US_sounds_A[index_USA])\n",
    "                    if List == stim_day3: \n",
    "                        List['US_cue'].append(cues[index_USA])\n",
    "                    CleanList['US'].append(US_A[index_USA])\n",
    "                    CleanList['valence'].append(1)\n",
    "                    CleanList['US_sound'].append(US_sounds_A[index_USA])\n",
    "                    if CleanList == stim_day3_clean: \n",
    "                        CleanList['US_cue'].append(cues[index_USA])\n",
    "            else:\n",
    "                index_USN = US_N_i.pop(random.randint(0, len(US_N_i) - 1))\n",
    "                for List, CleanList in zip([stim_day1, stim_day2, stim_day3], [stim_day1_clean, stim_day2_clean, stim_day3_clean]):\n",
    "                    List['US'].append(US_N[index_USN])\n",
    "                    List['valence'].append(2)\n",
    "                    List['US_sound'].append(US_sounds_N[index_USN])\n",
    "                    if List == stim_day3:\n",
    "                        List['US_cue'].append(cues[index_USN+18])\n",
    "                    CleanList['US'].append(US_N[index_USN])\n",
    "                    CleanList['valence'].append(2)\n",
    "                    CleanList['US_sound'].append(US_sounds_N[index_USN])\n",
    "                    if CleanList == stim_day3_clean:\n",
    "                        CleanList['US_cue'].append(cues[index_USN+18])\n",
    "            \n",
    "            episode_nr += 1\n",
    "\n",
    "    # Create the DataFrames and take out the control conditions for day 2\n",
    "    day1 = pd.DataFrame(stim_day1)\n",
    "    stim_day2 = pd.DataFrame(stim_day2)\n",
    "    day2_new = stim_day2[~stim_day2['condition'].isin([3, 6])]\n",
    "    day2 = pd.DataFrame(day2_new)\n",
    "    day3 = pd.DataFrame(stim_day3)\n",
    "\n",
    "    day1_clean = pd.DataFrame(stim_day1_clean)\n",
    "    stim_day2_clean = pd.DataFrame(stim_day2_clean)\n",
    "    pd_day2_clean = stim_day2_clean[~stim_day2_clean['condition'].isin([3, 6])]\n",
    "    day2_clean = pd.DataFrame(pd_day2_clean)\n",
    "    day3_clean = pd.DataFrame(stim_day3_clean)\n",
    "\n",
    "    # Save the DataFrames to text and TSV files\n",
    "    if not os.path.exists('Stimsets/'):\n",
    "        os.makedirs('Stimsets/')\n",
    "\n",
    "    if os.path.exists(f'Stimsets/{version}_day1.txt') or os.path.exists(f'Stimsets/{version}_day2.txt') or os.path.exists(f'Stimsets/{version}_day3_Presentation.txt'):\n",
    "        print(f'{version} already exists, delete it first to overwrite')\n",
    "    else:\n",
    "        # Save the combined columns to text files\n",
    "        day1['combined'] = day1.apply(lambda row: ''.join(row.values.astype(str)), axis=1)\n",
    "        day2['combined'] = day2.apply(lambda row: ''.join(row.values.astype(str)), axis=1)\n",
    "        day3['combined'] = day3.apply(lambda row: ''.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "        with open(f'Stimsets/{version}_day1.txt', 'w') as f:\n",
    "            f.write('\\n'.join(day1['combined']) + '\\n')\n",
    "\n",
    "        with open(f'Stimsets/{version}_day2.txt', 'w') as f:\n",
    "            f.write('\\n'.join(day2['combined']) + '\\n')\n",
    "\n",
    "        with open(f'Stimsets/{version}_day3.txt', 'w') as f:\n",
    "            f.write('\\n'.join(day3['combined']) + '\\n')\n",
    "\n",
    "        # Save the clean DataFrames to TSV files\n",
    "        day1_clean.to_csv(f'Stimsets/{version}_day1.tsv', sep='\\t', index=False)\n",
    "        day2_clean.to_csv(f'Stimsets/{version}_day2.tsv', sep='\\t', index=False)\n",
    "        day3_clean.to_csv(f'Stimsets/{version}_day3.tsv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cues = [\"Bathtub\", \"Ribcage\", \"No lips\", \"Gimace\", \"Riverbank\", \"Black eye\",\n",
    "        \"Hood of car\", \"Weapon\", \"Fracture\", \"Toilet\", \"Eye\", \"Tongue\", \n",
    "        \"Debris\", \"Toe\", \"Tied up\", \"Saw\", \"Whip\", \"In the vet's hand\",\n",
    "        \"Crawling\", \"Leash\", \"Flame\", \"Ginger hair\", \"Lake\", \"Books\",\n",
    "        \"Road\", \"Forest\", \"Shoes\", \"Vegetables\", \"Slices\", \"Brush\",\n",
    "        \"In the sky\", \"Cheese\", \"Bike\", \"Tool\", \"Pile of fruit\", \"Shiny fur\"]    \n",
    "\n",
    "# Create the trials for 10 different versions, Important: Only create new versions if necessary. It will overwrite the existing files. \n",
    "\n",
    "for i in range(10):\n",
    "    version = \"version\" + str(i+1)\n",
    "    create_trials_total(version, cues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This was previous version, just for Presentation experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trials_old(version: str):\n",
    "    \"\"\"Create trials for the Episodic extinction experiment, using 5 different stimuli for each trial:\n",
    "        A context, two narrative stimuli (NSs), a conditioned stimulus (CS) and one unconditioned stimulus (US), with an associated sound.\n",
    "        \n",
    "        The experiment has 10 different conditions with 40 trials in total, thus each condition has 4 trials.\n",
    "        In half of the trials, the US is negative, in the other half it is neutral.\n",
    "        \n",
    "        On day 2, two conditions are omitted as a control and the remaining 8 conditions are repeated. \n",
    "        In half of the repeated trials, the original context is presented, in the other half a new context is presented.\n",
    "        \n",
    "        On day 3, all the stimuli are CSs are presented again as a cue. Memory tests are performed using this cue.\n",
    "        \n",
    "        As an overview for the conditions:\n",
    "        condition 1: continued conditioning, negative US, same context\n",
    "        condition 2: EXT, negative US, same context\n",
    "        condition 3: continued conditioning, negative US, new context\n",
    "        condition 4: EXT, negative US, new context\n",
    "        condition 5: control, negative US\n",
    "        condition 6: EXT, neutral US, same context\n",
    "        condition 7: continued conditioning, neutral US, same context\n",
    "        condition 8: EXT, neutral US, new context\n",
    "        condition 9: continued conditioning, neutral US, new context\n",
    "        condition 10: control, neutral US\n",
    "\n",
    "        \n",
    "        ToDo: Add \"\" to every value in the dictionary\"\"\"\n",
    "\n",
    "    # Define the index possibilities of the stimuli\n",
    "    contexts_i = list(range(56))\n",
    "    NS_i = list(range(40))\n",
    "    CS_i = list(range(40))\n",
    "    US_A_i = list(range(20))\n",
    "    US_N_i = list(range(20)) \n",
    "\n",
    "    # Create lists with all stimulus \n",
    "    contexts = sorted(glob.glob('stimulus_files/contexts_equalized/*A*'))[:56]\n",
    "    NS = sorted(glob.glob('stimulus_files/NS_equalized/A*'))[:40]\n",
    "    CS = sorted(glob.glob('stimulus_files/CS_equalized/*'))[:40]\n",
    "    US_A = sorted(glob.glob('stimulus_files/US_equalized/negative*'))[:20]\n",
    "    US_N = sorted(glob.glob('stimulus_files/US_equalized/neutral*'))[:20]\n",
    "    US_sounds_A = sorted(glob.glob('stimulus_files/USsounds/negative*'))[:20]\n",
    "    US_sounds_N = sorted(glob.glob('stimulus_files/USsounds/neutral*'))[:20]\n",
    "\n",
    "    # Remove the directory from the values within the lists\n",
    "    contexts = [stim.split(\"/\")[-1] for stim in contexts]\n",
    "    NS = [stim.split(\"/\")[-1] for stim in NS]\n",
    "    CS = [stim.split(\"/\")[-1] for stim in CS]\n",
    "    US_A = [stim.split(\"/\")[-1] for stim in US_A]\n",
    "    US_N = [stim.split(\"/\")[-1] for stim in US_N]\n",
    "    US_sounds_A = [stim.split(\"/\")[-1] for stim in US_sounds_A]\n",
    "    US_sounds_N = [stim.split(\"/\")[-1] for stim in US_sounds_N]\n",
    "\n",
    "    # Define the empty data dictionaries for each day\n",
    "    stim_day1 = {\n",
    "        'start': [], 'context': [], 'delim': [], 'NS': [], 'delim2': [], 'CS': [], 'delim3': [],\n",
    "        'US': [], 'delim4': [], 'US_sound': [], 'delim5': [], 'condition': [], 'delim6': [],\n",
    "        'trial': [], 'delim7': [], 'valence': [], 'delim8': [], 'epsiode_nr': [], 'end': []\n",
    "    }\n",
    "\n",
    "    stim_day2 = {\n",
    "        'start': [], 'context': [], 'delim': [], 'CS': [], 'delim2': [], 'US': [], 'delim3': [],\n",
    "        'US_sound': [], 'delim4': [], 'condition': [], 'delim5': [], 'trial': [], 'delim6': [],\n",
    "        'valence': [], 'delim7': [], 'epsiode_nr': [], 'end': []\n",
    "    }\n",
    "\n",
    "    stim_day3 = {\n",
    "        'start': [], 'NS': [], 'delim': [], 'CS': [], 'delim2': [], 'US': [], 'delim3': [],\n",
    "        'US_sound': [], 'delim4': [], 'condition': [], 'delim5': [], 'trial': [], 'delim6': [],\n",
    "        'valence': [], 'delim7': [], 'epsiode_nr': [], 'end': []\n",
    "    }\n",
    "\n",
    "    # Additional dictionaries without 'start', 'delim', or 'end'\n",
    "    stim_day1_clean = {key: [] for key in stim_day1 if key not in ['start', 'delim', 'delim2', 'delim3', 'delim4', 'delim5', 'delim6', 'delim7', 'delim8', 'end']}\n",
    "    stim_day2_clean = {key: [] for key in stim_day2 if key not in ['start', 'delim', 'delim2', 'delim3', 'delim4', 'delim5', 'delim6', 'delim7', 'end']}\n",
    "    stim_day3_clean = {key: [] for key in stim_day3 if key not in ['start', 'delim', 'delim2', 'delim3', 'delim4', 'delim5', 'delim6', 'delim7', 'end']}\n",
    "\n",
    "    episode_nr = 1  # Fixed episode number for all trials\n",
    "    # Fill the trials for each day using two for loops, to make 10 conditions with 4 trials each\n",
    "    for i in range(10):  # Repeat 10 times for 40 rows (10 * 4 = 40)\n",
    "\n",
    "        for j in range(4):  # Each iteration adds 10 rows\n",
    "\n",
    "            # Randomly select indexes for each stimulus set\n",
    "            index_C = contexts_i.pop(random.randint(0, len(contexts_i) - 1))\n",
    "            index_NS = NS_i.pop(random.randint(0, len(NS_i) - 1))\n",
    "            index_CS = CS_i.pop(random.randint(0, len(CS_i) - 1))\n",
    "            \n",
    "            for List, CleanList in zip([stim_day1, stim_day2, stim_day3], [stim_day1_clean, stim_day2_clean, stim_day3_clean]):\n",
    "                List['start'].append('{\"')\n",
    "                List['delim'].append('\", \"')\n",
    "                List['delim2'].append('\", \"')\n",
    "                List['delim3'].append('\", \"')\n",
    "                List['delim4'].append('\", \"')\n",
    "                List['delim5'].append('\", \"')\n",
    "                List['delim6'].append('\", \"')\n",
    "                List['delim7'].append('\", \"')\n",
    "                List['end'].append('\"},')\n",
    "                if 'delim8' in List:\n",
    "                    List['delim8'].append('\", \"')\n",
    "\n",
    "                List['condition'].append(i + 1)\n",
    "                List['trial'].append(j + 1)\n",
    "                List['epsiode_nr'].append(episode_nr)\n",
    "                CleanList['condition'].append(i + 1)\n",
    "                CleanList['trial'].append(j + 1)\n",
    "                CleanList['epsiode_nr'].append(episode_nr)\n",
    "\n",
    "                if List == stim_day1:\n",
    "                    List['context'].append(contexts[index_C])\n",
    "                    List['NS'].append(NS[index_NS])\n",
    "                    List['CS'].append(CS[index_CS])\n",
    "                    CleanList['context'].append(contexts[index_C])\n",
    "                    CleanList['NS'].append(NS[index_NS])\n",
    "                    CleanList['CS'].append(CS[index_CS])\n",
    "                if List == stim_day2:\n",
    "                    List['context'].append(contexts[index_C])\n",
    "                    List['CS'].append(CS[index_CS])\n",
    "                    CleanList['context'].append(contexts[index_C])\n",
    "                    CleanList['CS'].append(CS[index_CS])\n",
    "                if List == stim_day3:\n",
    "                    List['NS'].append(NS[index_NS])\n",
    "                    List['CS'].append(CS[index_CS])\n",
    "                    CleanList['NS'].append(NS[index_NS])\n",
    "                    CleanList['CS'].append(CS[index_CS])\n",
    "\n",
    "            # Alternating between US_A and US_N for each 5 iterations, and add US sounds for day1 and day2\n",
    "            if i < 5:\n",
    "                index_USA = US_A_i.pop(random.randint(0, len(US_A_i) - 1))\n",
    "                for List, CleanList in zip([stim_day1, stim_day2, stim_day3], [stim_day1_clean, stim_day2_clean, stim_day3_clean]):\n",
    "                    List['US'].append(US_A[index_USA])\n",
    "                    List['valence'].append(1)\n",
    "                    List['US_sound'].append(US_sounds_A[index_USA])\n",
    "                    CleanList['US'].append(US_A[index_USA])\n",
    "                    CleanList['valence'].append(1)\n",
    "                    CleanList['US_sound'].append(US_sounds_A[index_USA])\n",
    "            else:\n",
    "                index_USN = US_N_i.pop(random.randint(0, len(US_N_i) - 1))\n",
    "                for List, CleanList in zip([stim_day1, stim_day2, stim_day3], [stim_day1_clean, stim_day2_clean, stim_day3_clean]):\n",
    "                    List['US'].append(US_N[index_USN])\n",
    "                    List['valence'].append(2)\n",
    "                    List['US_sound'].append(US_sounds_N[index_USN])\n",
    "                    CleanList['US'].append(US_N[index_USN])\n",
    "                    CleanList['valence'].append(2)\n",
    "                    CleanList['US_sound'].append(US_sounds_N[index_USN])\n",
    "\n",
    "            if i % 10 + 1 in [3, 4, 8, 9]:  # If the condition is 3, 4, 8 or 9, change the context on day 2 \n",
    "                index_C = contexts_i.pop(random.randint(0, len(contexts_i) - 1))\n",
    "                stim_day2['context'][-1] = contexts[index_C]\n",
    "                stim_day2_clean['context'][-1] = contexts[index_C]\n",
    "            \n",
    "            episode_nr += 1\n",
    "\n",
    "    # Create the DataFrames and take out the control conditions for day 2\n",
    "    day1 = pd.DataFrame(stim_day1)\n",
    "    stim_day2 = pd.DataFrame(stim_day2)\n",
    "    day2_new = stim_day2[~stim_day2['condition'].isin([5, 10])]\n",
    "    day2 = pd.DataFrame(day2_new)\n",
    "    day3 = pd.DataFrame(stim_day3)\n",
    "\n",
    "    day1_clean = pd.DataFrame(stim_day1_clean)\n",
    "    stim_day2_clean = pd.DataFrame(stim_day2_clean)\n",
    "    pd_day2_clean = stim_day2_clean[~stim_day2_clean['condition'].isin([5, 10])]\n",
    "    day2_clean = pd.DataFrame(pd_day2_clean)\n",
    "    day3_clean = pd.DataFrame(stim_day3_clean)\n",
    "\n",
    "    # Save the DataFrames to text and TSV files\n",
    "    if not os.path.exists('Stimsets/'):\n",
    "        os.makedirs('Stimsets/')\n",
    "\n",
    "    if os.path.exists(f'Stimsets/{version}_day1.txt') or os.path.exists(f'Stimsets/{version}_day2.txt') or os.path.exists(f'Stimsets/{version}_day3_Presentation.txt'):\n",
    "        print(f'{version} already exists, delete it first to overwrite')\n",
    "    else:\n",
    "        # Save the combined columns to text files\n",
    "        day1['combined'] = day1.apply(lambda row: ''.join(row.values.astype(str)), axis=1)\n",
    "        day2['combined'] = day2.apply(lambda row: ''.join(row.values.astype(str)), axis=1)\n",
    "        day3['combined'] = day3.apply(lambda row: ''.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "        with open(f'Stimsets/{version}_day1.txt', 'w') as f:\n",
    "            f.write('\\n'.join(day1['combined']) + '\\n')\n",
    "\n",
    "        with open(f'Stimsets/{version}_day2.txt', 'w') as f:\n",
    "            f.write('\\n'.join(day2['combined']) + '\\n')\n",
    "\n",
    "        with open(f'Stimsets/{version}_day3.txt', 'w') as f:\n",
    "            f.write('\\n'.join(day3['combined']) + '\\n')\n",
    "\n",
    "        # Save the clean DataFrames to TSV files\n",
    "        day1_clean.to_csv(f'Stimsets/{version}_day1.tsv', sep='\\t', index=False)\n",
    "        day2_clean.to_csv(f'Stimsets/{version}_day2.tsv', sep='\\t', index=False)\n",
    "        day3_clean.to_csv(f'Stimsets/{version}_day3.tsv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psych_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
